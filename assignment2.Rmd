# Assignment 2
## Biomedical Data Science
## B159640

This Rmarkdown document provides answers, tables and graphs for the problems 1-4 and the R code used to obtain them. It is recommended to knit the Rmarkdown to HTML to obtain the intended formatting.

### Problem 1

Loading datasets `lipids.txt` and `lipid-classes.txt` required for problem 1
```{r}
# load library to work with data tables (also for all following problems)
library(data.table)

# import file and look up files
lipids.dt <- fread("lipids.txt", stringsAsFactors = TRUE)
lipid.classes.dt <- fread("lipid-classes.txt", stringsAsFactors = TRUE)
```

#### Problem 1 (a)

Using the lipid-class lookup table `lipid.classes.dt` to determine the lipid classes
of the lipid species listed `lipids.dt`, then adding a new lipid class colum to `lipids.dt`: 

Writing a function to classify the lipids in a given data table.
It accepts as input a data table `lipid.table`, a string `lipid.species.col` that specifies the column of `lipid.table` in which the lipid species are stored and a vector `lipid.classes`:
```{r}
classify.lipids <- function(lipid.table, lipid.species.col, lipid.classes){
  
  # add  class colum to lipid.table
  lipid.table[, lipid.class := NA]
  
  # loop over over all lipid classes
  
  for(lipid in lipid.classes){
    
    # create regex that finds all lipid class identifiers
    regex <- paste0("(\\s|^)[", 
                    toupper(substr(lipid,1,1)),
                    tolower(substr(lipid,1,1)),
                    "](",
                    toupper(substr(lipid,2,nchar(lipid))),
                    "|",
                    tolower(substr(lipid,2,nchar(lipid))),
                    ")(\\s|$)")
  
    # add new col lipid.class to lipid.dt
    
    lipid.species <- paste0(lipid.table, "$" , lipid.species.col)
    lipid.table[, lipid.class := ifelse(grepl(regex, lipid.species), lipid, lipid.class)]
  }
}
```

Classifying the lipids:
```{r}
classify.lipids(lipids.dt, "lipid.species" , lipid.classes.dt$CE)
```

Checking the results:
```{r}
head(lipids.dt, n = 15)
```

Ooops, the function fails to classify lipids species containing the label "CE".
There are two possible reasons for this:

* the label "CE" and "Cer" tag Cholesterylester
* the label "CE" is the identifier of an ester not listed in `lipid.classed.dt`

A quick google search leads to the [LIPID MAPS® Lipidomics Gateway website](https://www.lipidmaps.org/tools/ms/lm_mass_form.php)
which provides access to lipid nomenclature, databases, tools, protocols and many more to serve the international lipid research community.
Here, we learn that "CE" is the identifier for lipids belonging to the class Cholesterylester.
Hence, we add this label to `lipid.classes.dt`:
```{r}
lipid.classes.dt <- rbind(lipid.classes.dt, list("CE", "Cholesterylesters"))
```

Rerunning the classification function:
```{r}
classify.lipids(lipids.dt, "lipid.species" , lipid.classes.dt$CE)
```

Checking the results (again):
```{r}
head(lipids.dt, n = 15)
```

Merging the data tables and counting the number of lipids that fall in each class
```{r}
results.dt <- merge(lipids.dt, lipid.classes.dt, 
                    by.x = "lipid.class", 
                    by.y = "CE" )

table(results.dt$`Cholesterol esters`)
```

#### Problem 1 (b)

Computing the wald test statistic for each lipid species:
```{r}
results.dt[, "wald.test" := round((oddsratio-1)/se,3)]
```

Computing a p-value for each lipid species using the t distribution and append them to `results.dt`:
(Assuming that there were 288 patients in the dataset)
```{r}
results.dt[, "p.value(t-distn)" := signif(1-(pt(abs(wald.test), 277)-pt(-abs(wald.test), 277)),3)]
```

Computing a p-value for each lipid species using the normal distribution and append them to `results.dt`:
```{r}
results.dt[, "p.value(n-distn)" := signif(1-(pnorm(abs(wald.test))-pnorm(-abs(wald.test))),3)]
```

Checking the results:
```{r}
head(results.dt)
```

Providing some evidence to justify if the normal approximation is acceptable in this instance:
```{r}
hist(results.dt$wald.test, 
     breaks = 12,
     freq=F,
     main = "Distribution of Wald test statistic",
     xlab = "Wald test statistic",
     ylab = "Frequency")

lines(density(results.dt$wald.test), col="red")

lines(seq(-8, 8, by=.05), 
      dnorm(seq(-8, 8, by=.05), mean(results.dt$wald.test), sd(results.dt$wald.test)),
      col="blue")

legend("topright", c("observed density", "normal density"), col = c("red","blue"), lty = c(1,1))
```

Comparing the observed distribution with a normal distribution, it can be stated that the observed distribution deviates from the normal distn due to a higher leftwards-shifted peak and positive skewedness. However, the normal distribution is still acceptable as a rough approximation since we only care about the extreme values (tails or the distn) when computing a p-value and evaluating whether it is above or below a certain threshold (e.g. 0.05 or 0.01).

#### Problem (c)

Implementing a function `holm.bonferroni(results.dt, alpha)` that takes as input an data.table containing the hypotheses tested and corresponding p-values (`results.dt`) and the desired significance level (`alpha`).
It computes the family-wise error rate for all hypotheses and returns `fwer.results.dt`a data table containing the subset of the tested hypothesis that are significant according to the Holm-Bonferroni method (FWER < `alpha`) ordered by increasing p-value.
```{r}
holm.bonferroni <- function(results.dt, alpha=0.05){
  
  # sorting p-values: assuming hypotheses are in the first column, p-values in the second column
  # rename columns
  colnames(results.dt) <- c('hypothesis','p.value')
  # sort results by p-value
  setorder(results.dt, cols = 'p.value')
  
  # variables
  
  m <- length(results.dt$p.value)
  k <- 1
  
  # loop over all p-values
  for (p.k in results.dt$p.value){
    
    # reject yes/no?
    if(p.k < alpha/(m+1-k)){
      #print(k)
      #print(p.k)
      k <- k + 1
    }else{
      break
    }
  }

  # subset output data table
  fwer.results.dt <- results.dt[1:k-1,]
  
  # order output data table
  setorder(fwer.results.dt, cols = 'p.value')
  
  # return result
  return(fwer.results.dt)
}
```

#### Problem (d)

Implementing a similiar function `benjamini.hochberg(results.dt, q)` that takes as input an data.table containing the hypotheses tested and corresponding p-values (`results.dt`) and and a false discovery rate `q`.
It computes the false-discovery rate for all hypotheses and returns `fdr.results.dt` a data table containing the subset of hypothesis that are significant according to the Benjamini-Hochberg method (FDR < `q`) ordered by increasing p-value.
```{r}
benjamini.hochberg <- function(results.dt, q=0.05){
  
  # sorting p-values: assuming hypotheses are in the first column, p-values in the second column
  # rename columns
  colnames(results.dt) <- c('hypothesis','p.value')
  # sort results by p-value
  setorder(results.dt, cols = 'p.value')
  
  # variables
  
  m <- length(results.dt$p.value)
  k <- 1
  
  # loop over all p-values
  for (p.k in results.dt$p.value){
    
    # reject yes/no?
    if(p.k < (k/m*q)){
      #print(k)
      #print(p.k)
      k <- k + 1
    }else{
      break
    }
  }
  
  # subset output data table
  fdr.results.dt <- results.dt[1:k-1,]
  
  # order output data table
  setorder(fdr.results.dt, cols = 'p.value')
  
  # return result
  return(fdr.results.dt)
}
``` 

#### Problem 1 (e)

Producing a volcano plot using a different 

* colour for the lipid species that are significant after controlling for the family-wise error rate at α = 0.05 
* symbol to highlight the lipid species that are considered significant according to the Benjamini-Hochberg procedure at a false discovery rate of 1%
```{r}
# creating a data table `lipids.results.dt` that only contains the lipid species tested and their corresponding p-values (t-distn)
lipids.hypotheses.dt <- results.dt[,c('lipid.species','p.value(t-distn)')]

# determining significant subsets of lipid species according to both methods at given thresholds
fwer.dt <- holm.bonferroni(lipids.hypotheses.dt, 0.05)
fdr.dt <- benjamini.hochberg(lipids.hypotheses.dt, 0.01)

#plotting results
plot(log(results.dt$oddsratio), -log10(results.dt$`p.value(t-distn)`), 
     main="Volcano plot",
     xlab="log Odds Ratio", 
     ylab="-log10(p-value)", 
     col=ifelse(results.dt$`p.value(t-distn)` %in% fwer.dt$p.value , "#FFB900FF","#008EFFFF"),
     pch=ifelse(results.dt$`p.value(t-distn)` %in% fdr.dt$p.value, 17, 19),
     cex=ifelse((results.dt$`p.value(t-distn)` %in% fdr.dt$p.value | results.dt$`p.value(t-distn)` %in% fwer.dt$`p.value(t-distn)`), 1.2, 0.5),
     xlim=c(-1,1),
     ylim=c(0,14))

# adding legend
legend("topleft", c("FWER < 5%", "FWER ≥ 5%", "FDR < 1%", "FDR ≥ 1%"), 
       col = c("#FFB900FF","#008EFFFF", "grey", "grey"),
       lwd = c(3,3,1,1),
       lty = c(1,1,NA,NA),
       pch = c(NA,NA,2,1))
```

#### Problem 1 (f)

Applying the `holm.bonferroni()` function to data table `lipids.results.dt` in order to determine the significant subsets of lipid species according to Holm-Bonferroni method:
```{r}
holm.bonferroni(lipids.hypotheses.dt, 0.05) # significance level alpha = 0.05
```

Applying the `benjamini.hochberg()` function in order to determine the significant subsets of lipid species according to the Benjamini-Hochberg procedure:
```{r}
benjamini.hochberg(lipids.hypotheses.dt, 0.05)
```

Determining the subset of lipid species that are significant according to both the Holm-Bonferroni method and the Benjamini-Hochberg procedure:
```{r}
fwer.dt <- holm.bonferroni(lipids.hypotheses.dt, 0.05)
fdr.dt <- benjamini.hochberg(lipids.hypotheses.dt, 0.05)
intersect(fwer.dt$hypothesis, fdr.dt$hypothesis)
```

### Problem 2

Loading the breast cancer dataset `wdbc2.csv` required for problem 2
```{r}
bc.dt <- fread("wdbc2.csv", stringsAsFactors = TRUE)
```

#### Problem 2 (a)

Using package `caret`, creating a data partition so that the training set 
contains 70% of the observations (and of course setting the random seed to 1 beforehand):
```{r}
library(caret)
set.seed(1)
train.idx <- createDataPartition(bc.dt$diagnosis, p=0.7)$Resample1
```

Fitting both a ridge regression model and a lasso model on the training set in order to diagnose the type of tumour from the 30 imaging biomarkers contained in the dataset:

Loading the `glmnet` package for ridge regression and LASSO regularization:
```{r}
library(glmnet)
```

Function to transform a dataframe to a matrix as expected by the glmnet package:
```{r}
prepare.glmnet <- function(data, formula=~ .){
  
  # create the design matrix to deal correctly with factor variables, 
  # without losing rows containing NAs
  old.opts <- options(na.action='na.pass')
  x <- model.matrix(formula, data)
  options(old.opts)
  
  # remove the intercept column, as glmnet will add one by default 
  x <- x[, -match("(Intercept)", colnames(x))]
  return(x)
}
```

Transforming the data
```{r}
# predictors
x.bc.dt <- prepare.glmnet(bc.dt[,!"id"], ~ . - diagnosis) # exclude the outcome
# outcome
y.bc.dt <- bc.dt$diagnosis
```

Fit both a ridge regression model and a lasso model on the training set to diagnose the type of tumour from the 30 biomarkers
```{r}
# ridge regression
bc.ridge <- glmnet(x.bc.dt, y.bc.dt, alpha=0, subset = train.idx, family="binomial")
# lasso regression
bc.lasso <- glmnet(x.bc.dt, y.bc.dt, subset = train.idx, family="binomial")
```

Plotting the trajectories of the coefficients for various lambda λ:
```{r}
par(mfrow=c(1,2), mar=c(4,4,5,2))
plot(bc.ridge, main="Ridge trajectories")
plot(bc.lasso, main="Lasso trajectories")
```

For each model learning by cross-validation the penalty parameter λ that maximizes the AUC:
```{r}
# ridge regression
bc.cv.ridge <- cv.glmnet(x.bc.dt, y.bc.dt, subset = train.idx, family="binomial", type.measure="auc", alpha=0)
# lasso regression
bc.cv.lasso <- cv.glmnet(x.bc.dt, y.bc.dt, subset = train.idx, family="binomial", type.measure="auc")
```

Plotting the cross-validation curve:
```{r}
par(mfrow=c(1,2), mar=c(4,4,5,2)) 
plot(bc.cv.ridge, main="Ridge")
plot(bc.cv.lasso, main="Lasso") 
```

Penalty parameters λ that maximizes the AUC
```{r}
# ridge regression
bc.cv.ridge$lambda.min # this is admittedly counterintuitive
# lasso regression
bc.cv.lasso$lambda.min # this is admittedly counterintuitive
```

#### Problem 2 (b)

For both models fitted in (a) extract the AUCs corresponding to the optimal λ:

There are two ways to this:

1. the optimal λ maximzes the AUC, so we just extract the maximal AUC
```{r}
# ridge
max(bc.cv.ridge$cvm)
# lasso
max(bc.cv.lasso$cvm)
```

2. extract the AUC from the field `cvm` of the fit corresponding to the optimal λ (`lamda.min`) from the field `lambda` of the fit:
```{r}
# ridge
bc.cv.ridge$cvm[which(bc.cv.ridge$lambda == bc.cv.ridge$lambda.min)]
# lasso
bc.cv.lasso$cvm[which(bc.cv.lasso$lambda == bc.cv.lasso$lambda.min)]
```

Extracting the AUCs corresponding to all λ where the corresponding AUC is within 1 standard error of the maximum AUC.
```{r}
# ridge
bc.cv.ridge$cvm[which(bc.cv.ridge$lambda <= bc.cv.ridge$lambda.1se)]
# lasso
bc.cv.lasso$cvm[which(bc.cv.lasso$lambda <= bc.cv.lasso$lambda.1se)]
```

#### Problem 2 (c)
Creating a data table for each model that reports the choice of λ, the corresponding model size (number of non-zero parameters) and their training AUC (3 significant digits for floating point values):
```{r}
# ridge

bc.ridge.eval.dt <- data.table("λ" = signif(bc.cv.ridge$lambda,3),
                            "ModelSize" = bc.cv.ridge$nzero,
                            "AUC" = signif(bc.cv.ridge$cvm, 3))

# lasso 

bc.lasso.eval.dt <- data.table("λ" = signif(bc.cv.lasso$lambda,3),
                            "ModelSize" = bc.cv.lasso$nzero,
                            "AUC" = signif(bc.cv.lasso$cvm, 3))
```

Plotting the ridge regression results:
```{r}
# ridge
par(mfrow=c(1,3), mar=c(4,4,5,2))
plot(bc.ridge.eval.dt$λ, bc.ridge.eval.dt$ModelSize, 
     main = "A",
     xlab = 'λ',
     ylab = 'Number of non-zero parameters',
     col = "chartreuse3",
     cex = .5,
     ylim = c(0,30))
plot(bc.ridge.eval.dt$λ, bc.ridge.eval.dt$AUC, 
     main = "B",
     xlab = 'λ',
     ylab = 'AUC',
     col = "aquamarine4",
     cex = .5,
     ylim = c(.95, 1))
plot(bc.ridge.eval.dt$ModelSize, bc.ridge.eval.dt$AUC, 
     main = "C",
     xlab = 'Number of non-zero parameters',
     ylab = 'AUC', 
     col = "cadetblue3",
     cex = .5,
     xlim = c(0,30),
     ylim = c(.95, 1))
```

Plot A illustrates that the choice of the penalty parameter λ has no effect on the model size when fitting ridge regression model to the data.
This what we expect, since ridge regression can't shrink the coefficient of the model parameters to zero and hence can not exclude unsuitable variables from the prediction.
Plot B depicts the relationship between the penalty parameter λ and the AUC.
AUC increases with decreasing λ, suggesting that there's a moderately low penalty yields the best results in terms of the AUC.

```{r}
# lasso
par(mfrow=c(1,3), mar=c(4,4,5,2))
plot(bc.lasso.eval.dt$λ, bc.lasso.eval.dt$ModelSize, 
     main = "A",
     xlab = 'λ',
     ylab = 'Number of non-zero parameters',
     col = "chartreuse3",
     cex = .5,
     ylim = c(0,30))
plot(bc.lasso.eval.dt$λ, bc.lasso.eval.dt$AUC, 
     main = "B",
     xlab = 'λ',
     ylab = 'AUC',
     col = "aquamarine4",
     cex = .5,
     ylim = c(.95, 1))
plot(bc.lasso.eval.dt$ModelSize, bc.lasso.eval.dt$AUC, 
     main = "C",
     xlab = 'Number of non-zero parameters',
     ylab = 'AUC', 
     col = "cadetblue3",
     cex = .5,
     ylim = c(.95, 1))
```

We observe that model size decreases with increasing λ (Plot A), which is what we expect when applying a lasso regression model to the data.
The more severe the penalty, the more the parameter coefficients will shrink, eventually going down to zero for sufficiently large λ.
This leads to parameters being excluded from the model, which in return decreases the model size
Further, we see that the AUC has its peak at aroung λ = 0.02. 
The AUC sharply declines when decreasing λ, and exhibits a steep, but steady with three plateuas when increasing λ.
Plot C illustrates the "curse of dimensionality": reducing the number of explanatory variables can in itself reduce the error metric or increasing the number of explanatory variable will lead to a worse prediction, suggesting that the exluded variables provide little additional information or are redundant.

#### Problem 2 (d)

Performing backwards elimination (model `bc.sel.back`) on the same training set derived at point (a) and

```{r}
# standardize variables to obtain standardized coefficients
bc.sd1.dt <- copy(bc.dt)
covar.variables <- colnames(bc.dt[, -c("id", "diagnosis")]) # exclude non covariate columns
bc.sd1.dt[, (covar.variables) := lapply(.SD, function(x) x / sd(x, na.rm = TRUE)), .SDcols = covar.variables]

# load library for stepwise regression
library(MASS)

# fit logistic regression model with all training split from (a)
bc.full.model <- glm(diagnosis ~ ., data=bc.sd1.dt[, !"id"], subset = train.idx, family="binomial") # all variables except the patient id

bc.sel.back <- stepAIC(bc.full.model, direction="back")
```

Reporting the variables selected and their standardized regression coefficients 
in decreasing order of the absolute value of their standardized regression coefficient:
```{r}
# create data table with variable and regression coefs
bc.sel.back.coefs <- data.table("variable" = rownames(coef(summary(bc.sel.back)))[-1],
                               "stdz.coef" = signif(coef(summary(bc.sel.back))[-1,1]))

# sort by absolute value of their standardized regression coefficient (in decreasing order)
bc.sel.back.coefs <- bc.sel.back.coefs[order(-abs(bc.sel.back.coefs$stdz.coef))]

#show results
bc.sel.back.coefs
```

#### Problem 2 (e)

Repeating the same analysis of point (d) by using stepwise selection (model `bc.sel.forw`) starting from the null model:
```{r}
bc.train.dt <- bc.sd1.dt[, !"id"][train.idx]

# fit logistic regression model with all the training split from (a)
bc.null.model <- glm(diagnosis ~ 1, data=bc.sd1.dt[, !"id"], subset = train.idx, family="binomial") # only the intercept

# perform stepwise regression (forward selection)
bc.step.select <- step(bc.null.model, scope=list(upper=bc.full.model),direction="both")
```

Reporting the variables selected and their standardized regression coefficients 
in decreasing order of the absolute value of their standardized regression coefficient:
```{r}
# create data table with variable and stdz coefs
bc.step.select.coefs <- data.table("variable" = rownames(coef(summary(bc.step.select)))[-1],
                                "stdz.coef" = signif(coef(summary(bc.step.select))[-1,1]))

# sort coefs by absolute value of their standardized regression coefficient (in decreasing order)
bc.step.select.coefs <- bc.step.select.coefs[order(-abs(bc.step.select.coefs$stdz.coef))]

# view results
bc.step.select.coefs
```

**Did at any point in this procedure occur that a variable entered the model and was later on discarded? If so which?**

When tracing back all steps of stepwise selection process, it can be observed that variables `perimeter.worst`, `texture` and `area.stderr` where initially added to the model, but were later discarded.

#### Problem 2 (f)

Comparing the goodness of fit of model `bc.sel.back`(backwards elimination) and model `bc.step.select` (stepwise selection):
Given that the two models are not nested, we cannot use a likelihood ratio test.
But we can still use the AIC and BIC:

```{r}
#AIC
AIC(bc.sel.back)
AIC(bc.step.select)
```

Since model `bc.sel.back` has lower AIC, so it’s tempting to say it's the better model.
BUT:
```{r}
#BIC
BIC(bc.sel.back)
BIC(bc.step.select)
```

Model `bc.step.select` has lower BIC (which applies a stronger penalty for the use of additional predictors than AIC).
So depending on what criteria you judge, `bc.sel.back` or `bc.step.select` will be the better model.
However, since the AICs are almost identical, it does not come as a surprise that the BIC of `bc.step.select` is lower, since it has far fewer predictors.

#### Problem 2 (g)

Using model `bc.sel.back` and model `bc.step.select` to predict observations in the training set, and from that compute the training AUC:

```{r}
# import package to compute and plot area under the curve
library(pROC)

# make predictions
bc.sel.back.pred <- predict(bc.sel.back, newdata=bc.dt[train.idx], type="response")
bc.step.select.pred <- predict(bc.step.select, newdata=bc.dt[train.idx], type="response")

# plot AUCs
par(mfrow=c(1,1))
roc(bc.dt$diagnosis[train.idx], bc.sel.back.pred, plot=TRUE, legacy.axes=TRUE, main = "Training AUCs", col="brown2", lwd = 1.3)
roc(bc.dt$diagnosis[train.idx], bc.step.select.pred, plot=TRUE, legacy.axes=TRUE, add=TRUE, col="deepskyblue3", lwd = 1.3)
legend("bottomright", c("backwards elimination: AUC = 0.992", "stepwise selection: AUC = 0.989"), fill = c("brown2","deepskyblue3"))
```

#### Problem 2 (h)

Using the four models to predict the outcome for the observations in the test set (using the lambda at 1 standard error for the ridge and lasso models)
```{r}
bc.sel.back.pred.obs <- predict(bc.sel.back, newdata=bc.dt[-train.idx,], type="response")
bc.step.select.pred.obs <- predict(bc.step.select, newdata=bc.dt[-train.idx,], type="response")
bc.ridge.pred.obs <- predict(bc.cv.ridge, newx = x.bc.dt[-train.idx,], type="response", s=bc.cv.ridge$lambda.1se)
bc.lasso.pred.obs <- predict(bc.cv.lasso, newx = x.bc.dt[-train.idx,], type="response", s=bc.cv.lasso$lambda.1se)
```

Plotting the ROC curves of these models (on the same plot, using different colours) and reporting their test AUCs:
```{r}
# plot test AUCs
par(mfrow=c(1,1))
roc(bc.dt$diagnosis[-train.idx], bc.sel.back.pred.obs, plot=TRUE, legacy.axes=TRUE, main = "Test AUCs",
    col="#FF9900FF", lwd = 3)
roc(bc.dt$diagnosis[-train.idx], bc.step.select.pred.obs, plot=TRUE, legacy.axes=TRUE, 
    add=TRUE, col="#33FF00FF", lwd = 3)
roc(bc.dt$diagnosis[-train.idx], as.vector(bc.ridge.pred.obs), plot=TRUE, legacy.axes=TRUE, 
    add=TRUE, col="#0066FFFF", lwd = 3)
roc(bc.dt$diagnosis[-train.idx], as.vector(bc.lasso.pred.obs), plot=TRUE, legacy.axes=TRUE, 
    add=TRUE, col="#FF0099FF", lwd = 3)
legend("bottomright", c("backwards elimination: AUC = 0.991", "stepwise selection: AUC = 0.992", "ridge regression: AUC = 0.988", "lasso regression: AUC = 0.992"), 
       fill = c("#FF9900FF","#33FF00FF","#0066FFFF", "#FF0099FF"))
```

Comparing the training AUCs with the test AUCs:
```{r}
# predict the outcome of lasso and ridge for the observations in the training data set
# (using the lambda at 1 standard error for the ridge and lasso models)
bc.ridge.pred <- predict(bc.cv.ridge, newx = x.bc.dt[train.idx,], type="response", s=bc.cv.ridge$lambda.1se)
bc.lasso.pred <- predict(bc.cv.lasso, newx = x.bc.dt[train.idx,], type="response", s=bc.cv.lasso$lambda.1se)

# plot training AUCc
par(mfrow=c(1,1))
roc(bc.dt$diagnosis[train.idx], bc.sel.back.pred, plot=TRUE, legacy.axes=TRUE, main = "Training AUCs",
    col="#FF9900FF", lwd = 3)
roc(bc.dt$diagnosis[train.idx], bc.step.select.pred, plot=TRUE, legacy.axes=TRUE, 
    add=TRUE, col="#33FF00FF", lwd = 3)
roc(bc.dt$diagnosis[train.idx], as.vector(bc.ridge.pred), plot=TRUE, legacy.axes=TRUE, 
    add=TRUE, col="#0066FFFF", lwd = 3)
roc(bc.dt$diagnosis[train.idx], as.vector(bc.lasso.pred), plot=TRUE, legacy.axes=TRUE, 
    add=TRUE, col="#FF0099FF", lwd = 3)
legend("bottomright", 
       c("backwards elimination: AUC = 0.992", "stepwise selection: AUC = 0.989", "ridge regression: AUC = 0.972", "lasso regression: AUC = 0.978"), 
       fill = c("#FF9900FF","#33FF00FF","#0066FFFF", "#FF0099FF"))
```

Commenting on the overfitting of each model:

For ridge regression model the training AUC was smaller than the test AUC (0.978 vs. 0.988), meaning is fits the testing data better than the training data, hence its definetly not overfitted. 
The Lasso regression model fits the training and the testing data equally well, which is reflected in the training and testing AUCs of 0.992 and 0.992, respectively.
In contrast to backwards elimination, for which produces the training AUC was larger than the test AUC (0.992 vs. 0.991), that the model os overfitted to the training data (meaning that it reduces the bias of the model at the cost of a higher variance).
For the stepwise selection model, however, the training AUCs is smaller than the testing AUCs, suggesting the model has a higher bias, but lower variance and hence is not overfitted and generalises better.

### Problem 3

#### Problem 3 (a)

Reading file `GDM.raw.txt` into a data table named `gdm.dt`, and storing rsID and coded allele in a separate data table calle `snp.allele.dt`

```{r}
# read file
gdm.dt <- fread("GDM.raw.txt")

# install package "stringr" if not installed already

if(!"stringr" %in% rownames(installed.packages())) {
  install.packages("stringr") # functions for making regex stuff super intuitive
}

library(stringr) # functions for intuitive working with regex

# define regex
rsID.regex <- "rs\\d+" # to capture rsID
reference.allele.regex <- "[ACGT]$" # to capture the reference allele


rsIDs <- str_extract(colnames(gdm.dt)[-c(1,2,3)], rsID.regex) # extract rsIDs (exclude colums 1-3: id, sex, pheno)
reference.alleles <- str_extract(colnames(gdm.dt)[-c(1,2,3)], reference.allele.regex) # extract all reference alleles (exclude colums 1-3: id, sex, pheno)

# store rsIDs and reference alleles in data table
snp.allele.dt <- data.table( "snp.name" = colnames(gdm.dt)[-c(1,2,3)],
                             "rsID" = str_extract(colnames(gdm.dt)[-c(1,2,3)], rsID.regex),
                             "reference.allele" = str_extract(colnames(gdm.dt)[-c(1,2,3)], reference.allele.regex))

# check results
head(snp.allele.dt)
```


Imputing missing values in `gdm.dt`` according to SNP-wise average allele count:
```{r}
# check number of NAs
table(is.na(gdm.dt))

# SNP-wise imputation
for (colnm in colnames(gdm.dt[,-1])) {
  gdm.dt[[colnm]][is.na(gdm.dt[[colnm]])] <- mean(gdm.dt[[colnm]], na.rm = TRUE)
}

# check numbers of NAs
table(is.na(gdm.dt))
```

#### Problem 3 (b)

Writing a function `univ.glm.test(x, y, order=FALSE)`
where x is a data table of SNPs, y is a binary outcome vector, and order is either TRUE or FALSE: 
Tthe function should fit a logistic regression model for each SNP in x and return a data table containing:

* SNP names, 
* regression coefficients, 
* odds ratios, 
* standard errors 
* and p-values. 

If order is set to TRUE, the output data table should be ordered by increasing p-value.


```{r}
univ.glm.test <- function( data, outcome, order=FALSE){
  
  output <- NULL
  
  # loop over all SNPs
  
  for (snp in 1:length(data)){
    
    # assertion check
    stopifnot(length(outcome) == length(data[[snp]]))
    
    # fit logistic regression model
    log.regr <- glm(outcome ~ data[[snp]], family = "binomial")
    
    # regression model summary with beta, std.error and p.value
    log.regr.summary <- data.table(signif(coef(summary(log.regr)),3))[-1,-3] # exclude intercept and t-value
    
    # add SNP summary to output table
    output <- rbind(output, log.regr.summary)
    
  }

  # add column of SNP IDs
  output <- cbind(snp.allele.dt$snp.name, output)
  
  # add colnames to output table
  colnames(output) <- c("snp","beta", "std.error", "p.value")
  
  # compute odds ratio
  output[, odds.ratio := signif(exp(beta),3)]
  
  if(order == TRUE){
    
    # sort output by increasing p-value
    setorder(output, p.value)
  }
  
  return(output)

}
```

#### Problem 3 (c)

Using function `univ.glm.test()` to run an association study for all the SNPs in `gdm.dt` against having gestational diabetes (column “pheno”):

```{r}
gdm.snp.dt <- univ.glm.test(gdm.dt[,!c("ID","sex","pheno")], gdm.dt$pheno, order = TRUE)

# check results
head(gdm.snp.dt)
```

For the SNP that is most strongly associated to increased risk of gestational diabetes 
reporting the summary statistics from the GWAS as well as the 95% and 99% confidence intervals on the odds ratio:
```{r}
# summary statistic
gdm.snp.dt[p.value == min(p.value)]

beta1 <- gdm.snp.dt[beta == max(beta), beta]
se1 <- gdm.snp.dt[beta == max(beta), std.error]

# 95% CI
round(exp(beta1 + 1.96 * se1 * c(-1, 1)), 3)

# 99% CI
round(exp(beta1 + 2.58 * se1 * c(-1, 1)), 3)
```


For the SNP with most significant protective effect reporting the summary statistics from the GWAS as well as the 95% and 99% confidence intervals on the odds ratio:
```{r}
# summary statistic
gdm.snp.dt[odds.ratio == min(odds.ratio)]

beta2 <- gdm.snp.dt[odds.ratio == min(odds.ratio), beta]
se2 <- gdm.snp.dt[odds.ratio == min(odds.ratio), std.error]

# 95% CI
round(exp(beta2 + 1.96 * se2 * c(-1, 1)), 3)

# 99% CI
round(exp(beta2 + 2.58 * se2 * c(-1, 1)), 3)
```

#### Problem 3 (d)

Merging the GWAS results with the table of gene names provided in file `GDM.annot.txt`
```{r}
# read GDM.annot.txt
gdm.annot.dt <- fread("GDM.annot.txt")

# merge with allele information
gdm.gwas.dt <- merge(snp.allele.dt, gdm.annot.dt,
                     by.x = "rsID", by.y = "snp")

# merge GWAS results
gdm.gwas.dt <- merge(gdm.gwas.dt, gdm.snp.dt,
                     by.x = "snp.name", by.y = "snp")

# convert pos column to numeric
gdm.gwas.dt[, pos := as.numeric(pos)]
```

Reporting SNP name, effect allele, chromosome number and corresponding gene name for all SNPs that have p-value < 10^−4:
```{r}
# filter results by p-value
hit.snp.dt <- gdm.gwas.dt[p.value < 10^-4]

# select columns
hit.snp.dt[,c(1,5,2,4)]
```

For all hit SNPs reporting all gene names that are within a 1Mb window from the SNP position on the same chromosome:
```{r}
# hit no.1
gdm.gwas.dt[chrom == hit.snp.dt$chrom[1]][pos >= hit.snp.dt$pos[1] - 1000000 & pos <= hit.snp.dt$pos[1] + 1000000]$gene

# hit.no.2
gdm.gwas.dt[chrom == hit.snp.dt$chrom[2]][pos >= hit.snp.dt$pos[2] - 1000000 & pos <= hit.snp.dt$pos[2] + 1000000]$gene
```

#### Problem (e)

Building a weighted genetic risk score that includes all SNPs with p-value < 10−4,
a second score with all SNPs with p-value < 10−3, 
and a third score that only includes SNPs on the FTO gene:

```{r}
# ensure that the ordering of SNPs is respected
gdm.gwas.dt <- gdm.gwas.dt[match(colnames(gdm.dt)[-c(1,2,3)], gdm.gwas.dt$snp.name),]

# assertion check that the ordering of SNPs is respected
stopifnot(colnames(gdm.dt)[-c(1,2,3)] == gdm.gwas.dt$snp.name)

# score 1: p.value < 10^-4
gdm1.snp <- gdm.gwas.dt[p.value < 1e-4]
gdm1.grs <- gdm.dt[, .SD, .SDcols = gdm.gwas.dt[p.value < 1e-4]$snp.name]
gdm1.weighted.grs <- as.matrix(gdm1.grs) %*% gdm1.snp$beta

# score 2: p.value < 10^-3
gdm2.snp <- gdm.gwas.dt[p.value < 1e-3]
gdm2.grs <- gdm.dt[, .SD, .SDcols = gdm.gwas.dt[p.value < 1e-3]$snp.name]
gdm2.weighted.grs <- as.matrix(gdm2.grs) %*% gdm2.snp$beta

# score 3: SNP on the FTO gene
gdm3.snp <- gdm.gwas.dt[gene == "FTO"]
gdm3.grs <- gdm.dt[, .SD, .SDcols = gdm.gwas.dt[gene == "FTO"]$snp.name]
gdm3.weighted.grs <- as.matrix(gdm3.grs) %*% gdm3.snp$beta
```

Adding the three scores as columns to the `gdm.dt` data table:
```{r}
gdm.dt$p4.score <- gdm1.weighted.grs
gdm.dt$p3.score <- gdm2.weighted.grs
gdm.dt$FTO.score <- gdm3.weighted.grs
```

Fitting the three scores in separate logistic regression models to test their association  with gestational diabetes:
```{r}
# score 1: SNPs with p.value < 10^-4
p4.score.log.regr <- glm(pheno ~ p4.score, data = gdm.dt, family = "binomial")
# score 2: SNPs with p.value < 10^-3
p3.score.log.regr <- glm(pheno ~ p3.score, data = gdm.dt, family = "binomial")
# score 3: SNPs on the FTO gene
FTO.score.log.regr <- glm(pheno ~ FTO.score, data = gdm.dt, family = "binomial")
```

```{r}
# function to fit logistic regression models and calculate odds ratio, 95% CI l and p-value
fit.score.model <- function(formula, data){

  # fit logistic regression model  
  log.regr <- glm(formula, data = data, family = "binomial")
  
  # compute odds ratio, 95% CI and p-value
  odds.ratio <- exp(coef(summary(log.regr))[2,1])
  CI.lower <- exp(confint(log.regr)[2,1])
  CI.upper <- exp(confint(log.regr)[2,2])
  p.value <- coef(summary(log.regr))[2,4]
  
  # brief summary table of the summary statistics
  gdm.grs.dt <- data.table(rbind(NULL, c(round(odds.ratio,3), round(CI.lower,3), round(CI.upper,3), signif(p.value,3))))
  colnames(gdm.grs.dt) <- c("odds.ratio","2.5%","97.5%","p.value")
  
  return(gdm.grs.dt)
}
```

Reporting odds ratio, 95% CI and p-value for each model/score:
```{r}
# function to calculate odds ratio, 95% CI l and p-value for a logistic regression model
model.stats <- function(model){

  # compute odds ratio, 95% CI and p-value
  odds.ratio <- exp(coef(summary(model))[2,1])
  CI.lower <- exp(confint(model)[2,1])
  CI.upper <- exp(confint(model)[2,2])
  p.value <- coef(summary(model))[2,4]
  
  # brief summary table of the summary statistics
  gdm.grs.dt <- data.table(rbind(NULL, c(round(odds.ratio,3), round(CI.lower,3), round(CI.upper,3), signif(p.value,3))))
  colnames(gdm.grs.dt) <- c("odds.ratio","2.5%","97.5%","p.value")
  
  return(gdm.grs.dt)
}

# score 1: SNPs with p.value < 10^-4
model.stats(p4.score.log.regr)
# score 2: SNPs with p.value < 10^-3
model.stats(p3.score.log.regr)
# score 3: SNPs on the FTO gene
model.stats(FTO.score.log.regr)
```

#### Problem 3 (f)

Reading the file `GDM.test.txt` into variable `gdm.test.dt`:
```{r}
gdm.test.dt <- fread("GDM.test.txt", stringsAsFactors = TRUE)
```

For the set of patients in `gdm.test.dt`, computing the three genetic risk scores as defined at point (e) using the same set of SNPs and corresponding weights:
```{r}
# ensure that the ordering of SNPs is respected
gdm.gwas.dt <- gdm.gwas.dt[match(colnames(gdm.test.dt)[-c(1,2,3)], gdm.gwas.dt$rsID),]

# assertion check that the ordering of SNPs is respected
stopifnot(colnames(gdm.test.dt)[-c(1,2,3)] == gdm.gwas.dt$rsID)

# score 1: p.value < 10^-4
gdm.test1.grs <- gdm.test.dt[, .SD, .SDcols = gdm.gwas.dt[p.value < 1e-4]$rsID]
gdm.test1.weighted.grs <- as.matrix(gdm.test1.grs) %*% gdm1.snp$beta

# score 2: p.value < 10^-3
gdm.test2.grs <- gdm.test.dt[, .SD, .SDcols = gdm.gwas.dt[p.value < 1e-3]$rsID]
gdm.test2.weighted.grs <- as.matrix(gdm.test2.grs) %*% gdm2.snp$beta

# score 3: SNP on the FTO gene
gdm.test3.grs <- gdm.test.dt[, .SD, .SDcols = gdm.gwas.dt[gene == "FTO"]$rsID]
gdm.test3.weighted.grs <- as.matrix(gdm.test3.grs) %*% gdm3.snp$beta
```

Adding the three scores as columns to `gdm.test.dt` (hint: use the same column names as before):
```{r}
gdm.test.dt$p4.score <- gdm.test1.weighted.grs
gdm.test.dt$p3.score <- gdm.test2.weighted.grs
gdm.test.dt$FTO.score <- gdm.test3.weighted.grs
```

#### Problem 3 (g)

Using the logistic regression models fitted at point (e) to predict the outcome of patients in `gdm.test.dt`:
```{r}
p4.score.pred <- predict(p4.score.log.regr, gdm.test.dt, type="response")
p3.score.pred <- predict(p3.score.log.regr, gdm.test.dt, type="response")
FTO.score.pred <- predict(FTO.score.log.regr, gdm.test.dt, type="response")
```

Computing the test log-likelihood for the predicted probabilities from the three genetic risk score models:
(using the binomial likelihood function I assumed)
```{r}
# for score 1: p.value < 10^-4
p4.score.pred.loglik <- sum(log(p4.score.pred[gdm.test.dt$pheno == 1])) + sum(log(1-p4.score.pred[gdm.test.dt$pheno == 0]))
p4.score.pred.loglik

# for score 2: p.value < 10^-3
p3.score.pred.loglik <- sum(log(p3.score.pred[gdm.test.dt$pheno == 1])) + sum(log(1-p3.score.pred[gdm.test.dt$pheno == 0]))
p3.score.pred.loglik

# for score 3: FTO gene
FTO.score.pred.loglik <- sum(log(FTO.score.pred[gdm.test.dt$pheno == 1])) + sum(log(1-FTO.score.pred[gdm.test.dt$pheno == 0]))
FTO.score.pred.loglik
```

#### Problem 3 (h)

Performing a meta-analysis of `GDM.study2.txt` containing the summary statistics from a different study on the same set of SNPs and the results obtained at point (c) and producing a summary of the meta-analysis results for the set of SNPs with meta-analysis p-value < 10^−4 sorted by increasing p-value:
```{r}
# datasets for meta analysis
gdm.gwas2.dt <- fread("GDM.study2.txt")
gdm.gwas1.dt <- gdm.gwas.dt

# harmonize datasets
gdm.gwas2.dt <- gdm.gwas2.dt[snp %in% gdm.gwas.dt$rsID]
gdm.gwas.dt <- gdm.gwas.dt[rsID %in% gdm.gwas2.dt$snp]

# order by chromosome and position
gdm.gwas2.dt <- gdm.gwas2.dt[match(gdm.gwas.dt$rsID, gdm.gwas2.dt$snp),]
stopifnot(all.equal(gdm.gwas.dt$rsID, gdm.gwas2.dt$snp))

# matching alleles
matching.alleles <- gdm.gwas.dt$reference.allele == gdm.gwas2.dt$effect.allele & gdm.gwas.dt$rsID == gdm.gwas2.dt$snp
# flipped alleles
flipping.alleles <- gdm.gwas.dt$reference.allele == gdm.gwas2.dt$other.allele & gdm.gwas.dt$rsID == gdm.gwas2.dt$snp
# unmatched alleles
unmatched.alleles <- matching.alleles == flipping.alleles
# summary
table(matching.alleles, flipping.alleles)

# ensure that the effect alleles correspond
beta1 <- gdm.gwas1.dt$beta
beta2 <- gdm.gwas2.dt$beta
beta2[flipping.alleles] <- -beta2[flipping.alleles]

# exclude unmatched SNPs
beta1 <- beta1[!unmatched.alleles]
beta2 <- beta2[!unmatched.alleles]

# inverse variance weighting
weight.gwas1 <- 1 / gdm.gwas1.dt$std.error[!unmatched.alleles]^2
weight.gwas2 <- 1 / gdm.gwas2.dt$se[!unmatched.alleles]^2

# computing the meta-analysis effect size 
beta.ma <- (weight.gwas1 * beta1 + weight.gwas2 * beta2) / (weight.gwas1 + weight.gwas2) 
se.ma <- sqrt(1 / (weight.gwas1 + weight.gwas2))

# calculating p-values of the meta-analysis
pval.ma <- 2 * pnorm(abs(beta.ma / se.ma), lower.tail=FALSE)

# creating summary data table of all SNPs and their corresponding p-values
ma.results.dt <- data.table("snp" = gdm.gwas1.dt$rsID[!unmatched.alleles],
                            "p.value" = pval.ma)

# select for SNPs with meta-analysis p-value < 10^−4
ma.results.dt <- ma.results.dt[p.value < 1e-4]
setorder(ma.results.dt, p.value)

# show 
head(ma.results.dt)
```

### Problem 4

Reading file `nki.csv` into a data table named `nki.dt` and creating `nki.genes.dt`, which only contains the gene expression values:
```{r}
nki.dt <- data.table(read.csv("nki.csv", stringsAsFactors = TRUE))
nki.genes.dt <- nki.dt[, -c(1,2,3,4,5,6)]
```

#### Problem 4 (a)

Computing the matrix of correlations between the gene expression variables, and displaying it so that a block structure is highlighted:
```{r}
# library for creating corrplots
library(corrplot)

# computing the correlation between the numerical colums `nki.dt`
nki.corr <- cor(nki.genes.dt, use="pairwise.complete")

# correlation plot
par(cex=0.4)
corrplot(nki.corr, order="hclust", diag=FALSE, tl.col="black",
         title="Correlation matrix (hierarchically clustered)",
         mar=c(0,2,4,0))

#corrplot(nki.corr, order="FPC", diag=FALSE, tl.col="black",
#         title="Correlation matrix (first principal component order)",
#         mar=c(0,2,4,0))

#corrplot(nki.corr, order="AOE", diag=FALSE, tl.col="black",
#         title="Correlation matrix (angular order of the eigenvectors)",
#         mar=c(0,2,4,0))
```

Writing some code to identify the unique pairs of (distinct) variables that have correlation coefficient greater than 0.80 in absolute value and reporting their correlation coefficients:
```{r}
corr.pairs <- NULL
# loop over correlation matrix
for (i in 1:nrow(nki.corr)){
  for (j in 1:ncol(nki.corr)){
    
    # ensure pairs of variables are unique and distinct
    if(i < j){
      # correlation coefficient greater than 0.80 in absolute value
      if(abs(nki.corr[i,j]) > 0.8){
        distinct.unique.pair <- c(rownames(nki.corr)[i], colnames(nki.corr)[j], signif(nki.corr[i,j],3))
        corr.pairs <- rbind(corr.pairs, distinct.unique.pair)
      }
    }
  }
}

# create summary data table
corr.pairs.dt <- data.table(corr.pairs)

# add rownames
colnames(corr.pairs.dt) <- c("gene1", "gene2", "correlation.coeff")

# sort by decreasing correlation coefficient
setorder(corr.pairs.dt, -correlation.coeff)

# view results
corr.pairs.dt
```

#### Problem 4 (b)

Running PCA (only over the columns containing gene expressions) so that it is possible to identify variable clusters. Producing a scatter plot of the projection of the predictors on the first two principal components and report the percentage of variance explained by the first two components.
```{r}
# assertion check whether there are any missing values in nki.dt
stopifnot(sum(is.na(nki.genes.dt)) == 0)

# perform PCA
pca.genes <- prcomp(t(nki.genes.dt), scale = TRUE)

# scatter plot: projection of the predictors on PC1 and PC2
par(cex=1)
plot(pca.genes$x[, 1:2], main="Projection of variables on the first 2 PCs", col = "firebrick", pch=19)

# compute percentage of variance explained by the first two components
perc.expl <- pca.genes$sdev^2 / sum(pca.genes$sdev^2) 
round(sum(perc.expl[1:2]),3)
```

The perentage of variance explained by PC1 and PC2 is 33.2%.

Starting from the PCA plot just produced, devising a simple rule PC2 < -10 that identifies the four gene expressions that are most different from the rest and reporting their names:
```{r}
pca.genes$x[which(pca.genes$x[,"PC2"] < -10), .SD]
```

#### Problem 4 (c)

Running PCA over the gene expression data again in order to derive a patient-wise summary of all gene expressions (dimensionality reduction), and only keeping the first 3 PCs:
```{r}
# perform pca
pca.patients <- prcomp(nki.genes.dt, scale = TRUE)

# plot results (outcomes are colour coded)
plot(pca.patients$x[, 1:2], main="Projection of patients on the first 2 PCs", 
     col = ifelse(nki.dt$Event == 1, "firebrick", "royalblue1"), pch=19, lwd=0.5)
legend("bottomleft", legend=c("No complications", "Complications"), col = c("royalblue1", "firebrick"), pch = 21)

# keep first 3 PCs
top3.PCs <- pca.patients$x[, 1:3]

# add top 3 PCs to nki.dt
nki.dt$PC1 <- top3.PCs[,1]
nki.dt$PC2 <- top3.PCs[,2]
nki.dt$PC3 <- top3.PCs[,3]
```

Testing if those PCs independently are associated with the outcome in unadjusted logistic regression models and in models adjusted for age, estrogen receptor and grade:
```{r}
# testing PC1
PC1.unadjusted.model <- glm(Event ~ PC1, data = nki.dt, family="binomial") 
PC1.adjusted.model <- glm(Event ~ PC1 + Age + EstrogenReceptor + Grade, data = nki.dt, family="binomial")
# beta and p.value of unadjusted model
coef(summary(PC1.unadjusted.model))[2,c(1,4)]
# beta and p.value of adjusted model
coef(summary(PC1.adjusted.model))[2,c(1,4)]

# testing PC2
PC2.unadjusted.model <- glm(Event ~ PC2, data = nki.dt, family="binomial")  # unadjusted model
PC2.adjusted.model <- glm(Event ~ PC2 + Age + EstrogenReceptor + Grade, data = nki.dt, family="binomial")  # adjusted model
# beta and p.value of unadjusted model
coef(summary(PC2.unadjusted.model))[2,c(1,4)]
# beta and p.value of adjusted model
coef(summary(PC2.adjusted.model))[2,c(1,4)]

# testing PC3
PC3.unadjusted.model <- glm(Event ~ PC3, data = nki.dt, family="binomial")   # unadjusted model
PC3.adjusted.model <- glm(Event ~ PC3 + Age + EstrogenReceptor + Grade, data = nki.dt, family="binomial")  # adjusted model
# beta and p.value of unadjusted model
coef(summary(PC3.unadjusted.model))[2,c(1,4)]
# beta and p.value of adjusted model
coef(summary(PC3.adjusted.model))[2,c(1,4)]
```

**Justifying the difference in results between unadjusted and adjusted models:**

Looking at the results it can be claimed that there is a significant assocation (p < 0.05) between PC1 and the insurgence of further complications after operation, as well as between PC3 and the insurgence of complications.
However, this is a hasty conclusion, given that the insurgence of operations for sure also depends on clinical parameters, such as age and tumor classification (estrogen recptor positive or negativ) and tumor staging (differentiation grade).
Hence, it is inevitable to test whether the association between the PCs of the gene expression and complications is independent of the most common clinical confounders.
In order to test this, we evaluated whether the association between PC1 and complications as well as PC3 and complications are still significant after adjusting for age, estrogen recptor status and differentiation grade, since these variables are known to affect the outcome.

After adjusting for these so called confounders, we can claim there is no significant association between P1 and complications after adjusting for age, estrogen recptor status and differentiation grade.
However, we can claim there is a significant association between P3 and complications even after adjusting for these confounders.
Hence, we can conclude that the association between PC3 and complications is independent of these confounders, wheras these PC1 and complications is not.
Lastly, there is no significant association between PC2 and complications and this does not change after adjusting for the confounders.

#### Problem 4 (d)

Fitting a lasso model to predict the binary outcome using all available predictors, learning the optimal penalty parameter that maximises the AUC.
Reporting the results (λ and AUC):
```{r}
# transforming the data
# predictors
x.nki.dt <- prepare.glmnet(nki.dt[,!c("PC1","PC2","PC3")], ~ . - Event) # exclude the outcome
# outcome
y.nki.dt <- nki.dt$Event
# set.seed
set.seed(1)

# fitting a lasso model and learning by cross-validation the penalty parameter λ that maximizes the AUC

nki.cv.lasso1 <- cv.glmnet(x.nki.dt, y.nki.dt, family="binomial", type.measure="auc")

# penalty parameters λ that maximizes the AUC
nki.cv.lasso1$lambda.min

# max AUC
nki.cv.lasso1$cvm[which(nki.cv.lasso1$lambda == nki.cv.lasso1$lambda.min)]
```

System run time on my machine was 0.013s.

Repeating the same procedure but this time penalising only to the gene expression variables, leaving all other covariates unpenalised.
Reporting the results (λ and AUC):
```{r}
nki.cv.lasso2 <- cv.glmnet(x.nki.dt, y.nki.dt, penalty.factor = c( rep(0, 5), rep(1,71)),
                          family="binomial", type.measure="auc")

# penalty parameters λ that maximizes the AUC
nki.cv.lasso2$lambda.min
# max AUC
nki.cv.lasso2$cvm[which(nki.cv.lasso2$lambda == nki.cv.lasso2$lambda.min)]
```

System run time on my machine was 0.033s.

**Can you explain the difference between the two models in terms of AUC?**
