# Assignment 2
## Biomedical Data Science
## B159640

This Rmarkdown document provides answers, tables and graphs for the problems 1-4 and the R code used to obtain them. It is recommended to knit the Rmarkdown to HTML to obtain the intended formatting.

### Problem 1

Loading datasets `lipids.txt` and `lipid-classes.txt` required for problem 1
```{r}
# load library to work with data tables (also for all following problems)
library(data.table)

# import file and look up files
lipids.dt <- fread("lipids.txt", stringsAsFactors = TRUE)
lipid.classes.dt <- fread("lipid-classes.txt", stringsAsFactors = TRUE)
```

#### Problem 1 (a)

Using the lipid-class lookup table `lipid.classes.dt` to determine the lipid classes
of the lipid species listed `lipids.dt`, then adding a new lipid class colum to `lipids.dt`: 

Writing a function to classify the lipids in a given data table.
It accepts as input a data table `lipid.table`, a string `lipid.species.col` that specifies the column of `lipid.table` in which the lipid species are stored and a vector `lipid.classes`:
```{r}
classify.lipids <- function(lipid.table, lipid.species.col, lipid.classes){
  
  # add  class colum to lipid.table
  lipid.table[, lipid.class := NA]
  
  # loop over over all lipid classes
  
  for(lipid in lipid.classes){
    
    # create regex that finds all lipid class identifiers
    regex <- paste0("(\\s|^)[", 
                    toupper(substr(lipid,1,1)),
                    tolower(substr(lipid,1,1)),
                    "](",
                    toupper(substr(lipid,2,nchar(lipid))),
                    "|",
                    tolower(substr(lipid,2,nchar(lipid))),
                    ")(\\s|$)")
  
    # add new col lipid.class to lipid.dt
    
    lipid.species <- paste0(lipid.table, "$" , lipid.species.col)
    lipid.table[, lipid.class := ifelse(grepl(regex, lipid.species), lipid, lipid.class)]
  }
}
```

Classifying the lipids:
```{r}
classify.lipids(lipids.dt, "lipid.species" , lipid.classes.dt$CE)
```

Checking the results:
```{r}
head(lipids.dt, n = 15)
```

Ooops, the function fails to classify lipids species containing the label "CE".
There are two possible reasons for this:

* the label "CE" and "Cer" tag Cholesterylester
* the label "CE" is the identifier of an ester not listed in `lipid.classed.dt`

A quick google search leads to the [LIPID MAPS® Lipidomics Gateway website](https://www.lipidmaps.org/tools/ms/lm_mass_form.php)
which provides access to lipid nomenclature, databases, tools, protocols and many more to serve the international lipid research community.
Here, we learn that "CE" is the identifier for lipids belonging to the class Cholesterylester.
Hence, we add this label to `lipid.classes.dt`:
```{r}
lipid.classes.dt <- rbind(lipid.classes.dt, list("CE", "Cholesterylesters"))
```

Rerunning the classification function:
```{r}
classify.lipids(lipids.dt, "lipid.species" , lipid.classes.dt$CE)
```

Checking the results (again):
```{r}
head(lipids.dt, n = 15)
```

Merging the data tables and counting the number of lipids that fall in each class
```{r}
results.dt <- merge(lipids.dt, lipid.classes.dt, 
                    by.x = "lipid.class", 
                    by.y = "CE" )

table(results.dt$`Cholesterol esters`)
```

#### Problem 1 (b)

Computing the wald test statistic for each lipid species:
```{r}
results.dt[, "wald.test" := round((oddsratio-1)/se,3)]
```

Computing a p-value for each lipid species using the t distribution and append them to `results.dt`:
(Assuming that there were 288 patients in the dataset)
```{r}
results.dt[, "p.value(t-distn)" := signif(1-(pt(abs(wald.test), 277)-pt(-abs(wald.test), 277)),3)]
```

Computing a p-value for each lipid species using the normal distribution and append them to `results.dt`:
```{r}
results.dt[, "p.value(n-distn)" := signif(1-(pnorm(abs(wald.test))-pnorm(-abs(wald.test))),3)]
```

Checking the results:
```{r}
head(results.dt)
```

Providing some evidence to justify if the normal approximation is acceptable in this instance:
```{r}
hist(results.dt$wald.test, 
     breaks = 12,
     freq=F,
     main = "Distribution of Wald test statistic",
     xlab = "Wald test statistic",
     ylab = "Frequency")

lines(density(results.dt$wald.test), col="red")

lines(seq(-8, 8, by=.05), 
      dnorm(seq(-8, 8, by=.05), mean(results.dt$wald.test), sd(results.dt$wald.test)),
      col="blue")

legend("topright", c("observed density", "normal density"), col = c("red","blue"), lty = c(1,1))
```

Comparing the observed distribution with a normal distribution, it can be stated that the observed distribution deviates from the normal distn due to a higher leftwards-shifted peak and positive skewedness. However, the normal distribution is still acceptable as a rough approximation since we only care about the extreme values (tails or the distn) when computing a p-value and evaluating whether it is above or below a certain threshold (e.g. 0.05 or 0.01).

#### Problem (c)

Implementing a function `holm.bonferroni(results.dt, alpha)` that takes as input an data.table containing the hypotheses tested and corresponding p-values (`results.dt`) and the desired significance level (`alpha`).
It computes the family-wise error rate for all hypotheses and returns `fwer.results.dt`a data table containing the subset of the tested hypothesis that are significant according to the Holm-Bonferroni method (FWER < `alpha`) ordered by increasing p-value.
```{r}
holm.bonferroni <- function(results.dt, alpha=0.05){
  
  # sorting p-values: assuming hypotheses are in the first column, p-values in the second column
  # rename columns
  colnames(results.dt) <- c('hypothesis','p.value')
  # sort results by p-value
  setorder(results.dt, cols = 'p.value')
  
  # variables
  
  m <- length(results.dt$p.value)
  k <- 1
  
  # loop over all p-values
  for (p.k in results.dt$p.value){
    
    # reject yes/no?
    if(p.k < alpha/(m+1-k)){
      #print(k)
      #print(p.k)
      k <- k + 1
    }else{
      break
    }
  }

  # subset output data table
  fwer.results.dt <- results.dt[1:k-1,]
  
  # order output data table
  setorder(fwer.results.dt, cols = 'p.value')
  
  # return result
  return(fwer.results.dt)
}
```

#### Problem (d)

Implementing a similiar function `benjamini.hochberg(results.dt, q)` that takes as input an data.table containing the hypotheses tested and corresponding p-values (`results.dt`) and and a false discovery rate `q`.
It computes the false-discovery rate for all hypotheses and returns `fdr.results.dt` a data table containing the subset of hypothesis that are significant according to the Benjamini-Hochberg method (FDR < `q`) ordered by increasing p-value.
```{r}
benjamini.hochberg <- function(results.dt, q=0.05){
  
  # sorting p-values: assuming hypotheses are in the first column, p-values in the second column
  # rename columns
  colnames(results.dt) <- c('hypothesis','p.value')
  # sort results by p-value
  setorder(results.dt, cols = 'p.value')
  
  # variables
  
  m <- length(results.dt$p.value)
  k <- 1
  
  # loop over all p-values
  for (p.k in results.dt$p.value){
    
    # reject yes/no?
    if(p.k < (k/m*q)){
      #print(k)
      #print(p.k)
      k <- k + 1
    }else{
      break
    }
  }
  
  # subset output data table
  fdr.results.dt <- results.dt[1:k-1,]
  
  # order output data table
  setorder(fdr.results.dt, cols = 'p.value')
  
  # return result
  return(fdr.results.dt)
}
``` 

#### Problem 1 (e)

Producing a volcano plot using a different 

* colour for the lipid species that are significant after controlling for the family-wise error rate at α = 0.05 
* symbol to highlight the lipid species that are considered significant according to the Benjamini-Hochberg procedure at a false discovery rate of 1%
```{r}
# creating a data table `lipids.results.dt` that only contains the lipid species tested and their corresponding p-values (t-distn)
lipids.hypotheses.dt <- results.dt[,c('lipid.species','p.value(t-distn)')]

# determining significant subsets of lipid species according to both methods at given thresholds
fwer.dt <- holm.bonferroni(lipids.hypotheses.dt, 0.05)
fdr.dt <- benjamini.hochberg(lipids.hypotheses.dt, 0.01)

#plotting results
plot(log(results.dt$oddsratio), -log10(results.dt$`p.value(t-distn)`), 
     main="Volcano plot",
     xlab="log Odds Ratio", 
     ylab="-log10(p-value)", 
     col=ifelse(results.dt$`p.value(t-distn)` %in% fwer.dt$p.value , "#FFB900FF","#008EFFFF"),
     pch=ifelse(results.dt$`p.value(t-distn)` %in% fdr.dt$p.value, 17, 19),
     cex=ifelse((results.dt$`p.value(t-distn)` %in% fdr.dt$p.value | results.dt$`p.value(t-distn)` %in% fwer.dt$`p.value(t-distn)`), 1.2, 0.5),
     xlim=c(-1,1),
     ylim=c(0,14))

# adding legend
legend("topleft", c("FWER < 5%", "FWER ≥ 5%", "FDR < 1%", "FDR ≥ 1%"), 
       col = c("#FFB900FF","#008EFFFF", "grey", "grey"),
       lwd = c(3,3,1,1),
       lty = c(1,1,NA,NA),
       pch = c(NA,NA,2,1))
```

#### Problem 1 (f)

Applying the `holm.bonferroni()` function to data table `lipids.results.dt` in order to determine the significant subsets of lipid species according to Holm-Bonferroni method:
```{r}
holm.bonferroni(lipids.hypotheses.dt, 0.05) # significance level alpha = 0.05
```

Applying the `benjamini.hochberg()` function in order to determine the significant subsets of lipid species according to the Benjamini-Hochberg procedure:
```{r}
benjamini.hochberg(lipids.hypotheses.dt, 0.05)
```

Determining the subset of lipid species that are significant according to both the Holm-Bonferroni method and the Benjamini-Hochberg procedure:
```{r}
fwer.dt <- holm.bonferroni(lipids.hypotheses.dt, 0.05)
fdr.dt <- benjamini.hochberg(lipids.hypotheses.dt, 0.05)
intersect(fwer.dt$hypothesis, fdr.dt$hypothesis)
```

### Problem 2

Loading the breast cancer dataset `wdbc2.csv` required for problem 2
```{r}
bc.dt <- fread("wdbc2.csv", stringsAsFactors = TRUE)
```

#### Problem 2 (a)

Using package `caret`, creating a data partition so that the training set 
contains 70% of the observations (and of course setting the random seed to 1 beforehand):
```{r}
library(caret)
set.seed(1)
train.idx <- createDataPartition(bc.dt$diagnosis, p=0.7)$Resample1
```

Fitting both a ridge regression model and a lasso model on the training set in order to diagnose the type of tumour from the 30 imaging biomarkers contained in the dataset:

Loading the `glmnet` package for ridge regression and LASSO regularization:
```{r}
library(glmnet)
```

Function to transform a dataframe to a matrix as expected by the glmnet package:
```{r}
prepare.glmnet <- function(data, formula=~ .){
  
  # create the design matrix to deal correctly with factor variables, 
  # without losing rows containing NAs
  old.opts <- options(na.action='na.pass')
  x <- model.matrix(formula, data)
  options(old.opts)
  
  # remove the intercept column, as glmnet will add one by default 
  x <- x[, -match("(Intercept)", colnames(x))]
  return(x)
}
```

Transforming the data
```{r}
# predictors
x.bc.dt <- prepare.glmnet(bc.dt[,!"id"], ~ . - diagnosis) # exclude the outcome
# outcome
y.bc.dt <- bc.dt$diagnosis
```

Fit both a ridge regression model and a lasso model on the training set to diagnose the type of tumour from the 30 biomarkers
```{r}
# ridge regression
bc.ridge <- glmnet(x.bc.dt, y.bc.dt, alpha=0, subset = train.idx, family="binomial")
# lasso regression
bc.lasso <- glmnet(x.bc.dt, y.bc.dt, subset = train.idx, family="binomial")
```

Plotting the trajectories of the coefficients for various lambda λ:
```{r}
par(mfrow=c(1,2), mar=c(4,4,5,2))
plot(bc.ridge, main="Ridge trajectories")
plot(bc.lasso, main="Lasso trajectories")
```

For each model learning by cross-validation the penalty parameter λ that maximizes the AUC:
```{r}
# ridge regression
bc.cv.ridge <- cv.glmnet(x.bc.dt, y.bc.dt, subset = train.idx, family="binomial", type.measure="auc", alpha=0)
# lasso regression
bc.cv.lasso <- cv.glmnet(x.bc.dt, y.bc.dt, subset = train.idx, family="binomial", type.measure="auc")
```

Plotting the cross-validation curve:
```{r}
par(mfrow=c(1,2), mar=c(4,4,5,2)) 
plot(bc.cv.ridge, main="Ridge")
plot(bc.cv.lasso, main="Lasso") 
```

Penalty parameters λ that maximizes the AUC
```{r}
# ridge regression
bc.cv.ridge$lambda.min # this is admittedly counterintuitive
# lasso regression
bc.cv.lasso$lambda.min # this is admittedly counterintuitive
```

#### Problem 2 (b)

For both models fitted in (a) extract the AUCs corresponding to the optimal λ:

There are two ways to this:

1. the optimal λ maximzes the AUC, so we just extract the maximal AUC
```{r}
# ridge
max(bc.cv.ridge$cvm)
# lasso
max(bc.cv.lasso$cvm)
```

2. extract the AUC from the field `cvm` of the fit corresponding to the optimal λ (`lamda.min`) from the field `lambda` of the fit:
```{r}
# ridge
bc.cv.ridge$cvm[which(bc.cv.ridge$lambda == bc.cv.ridge$lambda.min)]
# lasso
bc.cv.lasso$cvm[which(bc.cv.lasso$lambda == bc.cv.lasso$lambda.min)]
```

Extracting the AUCs corresponding to all λ where the corresponding AUC is within 1 standard error of the maximum AUC.
```{r}
# ridge
bc.cv.ridge$cvm[which(bc.cv.ridge$lambda <= bc.cv.ridge$lambda.1se)]
# lasso
bc.cv.lasso$cvm[which(bc.cv.lasso$lambda <= bc.cv.lasso$lambda.1se)]
```

#### Problem 2 (c)
Creating a data table for each model that reports the choice of λ, the corresponding model size (number of non-zero parameters) and their training AUC (3 significant digits for floating point values):
```{r}
# ridge

bc.ridge.eval.dt <- data.table("λ" = signif(bc.cv.ridge$lambda,3),
                            "ModelSize" = bc.cv.ridge$nzero,
                            "AUC" = signif(bc.cv.ridge$cvm, 3))

# lasso 

bc.lasso.eval.dt <- data.table("λ" = signif(bc.cv.lasso$lambda,3),
                            "ModelSize" = bc.cv.lasso$nzero,
                            "AUC" = signif(bc.cv.lasso$cvm, 3))
```

Plotting the ridge regression results:
```{r}
# ridge
par(mfrow=c(1,3), mar=c(4,4,5,2))
plot(bc.ridge.eval.dt$λ, bc.ridge.eval.dt$ModelSize, 
     main = "A",
     xlab = 'λ',
     ylab = 'Number of non-zero parameters',
     col = "chartreuse3",
     cex = .5,
     ylim = c(0,30))
plot(bc.ridge.eval.dt$λ, bc.ridge.eval.dt$AUC, 
     main = "B",
     xlab = 'λ',
     ylab = 'AUC',
     col = "aquamarine4",
     cex = .5,
     ylim = c(.95, 1))
plot(bc.ridge.eval.dt$ModelSize, bc.ridge.eval.dt$AUC, 
     main = "C",
     xlab = 'Number of non-zero parameters',
     ylab = 'AUC', 
     col = "cadetblue3",
     cex = .5,
     xlim = c(0,30),
     ylim = c(.95, 1))
```

Plot A illustrates that the choice of the penalty parameter λ has no effect on the model size when fitting ridge regression model to the data.
This what we expect, since ridge regression can't shrink the coefficient of the model parameters to zero and hence can not exclude unsuitable variables from the prediction.
Plot B depicts the relationship between the penalty parameter λ and the AUC.
AUC increases with decreasing λ, suggesting that there's a moderately low penalty yields the best results in terms of the AUC.

```{r}
# lasso
par(mfrow=c(1,3), mar=c(4,4,5,2))
plot(bc.lasso.eval.dt$λ, bc.lasso.eval.dt$ModelSize, 
     main = "A",
     xlab = 'λ',
     ylab = 'Number of non-zero parameters',
     col = "chartreuse3",
     cex = .5,
     ylim = c(0,30))
plot(bc.lasso.eval.dt$λ, bc.lasso.eval.dt$AUC, 
     main = "B",
     xlab = 'λ',
     ylab = 'AUC',
     col = "aquamarine4",
     cex = .5,
     ylim = c(.95, 1))
plot(bc.lasso.eval.dt$ModelSize, bc.lasso.eval.dt$AUC, 
     main = "C",
     xlab = 'Number of non-zero parameters',
     ylab = 'AUC', 
     col = "cadetblue3",
     cex = .5,
     ylim = c(.95, 1))
```

We observe that model size decreases with increasing λ (Plot A), which is what we expect when applying a lasso regression model to the data.
The more severe the penalty, the more the parameter coefficients will shrink, eventually going down to zero for sufficiently large λ.
This leads to parameters being excluded from the model, which in return decreases the model size
Further, we see that the AUC has its peak at aroung λ = 0.02. 
The AUC sharply declines when decreasing λ, and exhibits a steep, but steady with three plateuas when increasing λ.
Plot C illustrates the "curse of dimensionality": reducing the number of explanatory variables can in itself reduce the error metric or increasing the number of explanatory variable will lead to a worse prediction, suggesting that the exluded variables provide little additional information or are redundant.

#### Problem 2 (d)

Performing backwards elimination (model `bc.sel.back`) on the same training set derived at point (a) and

```{r}
# load library for stepwise regression
library(MASS)

# fit logistic regression model with all the training split from (a)
bc.full.model <- glm(diagnosis ~ ., data=bc.dt[, !"id"], subset = train.idx, family="binomial") # all variables except the patient id

# perform stepwise regression (backward elimination)
bc.sel.back <- stepAIC(bc.full.model, direction="back")
```

Reporting the variables selected and their standardized regression coefficients 
in decreasing order of the absolute value of their standardized regression coefficient:
```{r}
# function to compute standardized regression coefficients
stdz.coef <- function(model){
  b <- summary(model)$coef[-1,1]
  sx <- sapply(model$model[-1], sd)
  beta <-(3^(1/2))/pi * sx * b
  return(beta)
}

# create data table with variable and regression coefs
bc.sel.back.coefs <- data.table("variable" = rownames(coef(summary(bc.sel.back)))[-1],
                               "stdz.coef" = signif(stdz.coef(bc.sel.back)))

# sort by absolute value of their standardized regression coefficient (in decreasing order)
bc.sel.back.coefs <- bc.sel.back.coefs[order(-abs(bc.sel.back.coefs$stdz.coef))]

#show results
bc.sel.back.coefs
```

#### Problem 2 (e)

Repeating the same analysis of point (d) by using stepwise selection (model `bc.sel.forw`) starting from the null model:
```{r}
# fit logistic regression model with all the training split from (a)
bc.null.model <- glm(diagnosis ~ 1, data=bc.dt[, !"id"], subset = train.idx, family="binomial") # only the intercept

# perform stepwise regression (forward selection)
bc.sel.forw <- stepAIC(bc.null.model, scope=list(upper=bc.full.model),direction="forward")
```

Reporting the variables selected and their standardized regression coefficients 
in decreasing order of the absolute value of their standardized regression coefficient:
```{r}
# create data table with variable and stdz coefs
bc.sel.forw.coefs <- data.table("variable" = rownames(coef(summary(bc.sel.forw)))[-1],
                                "stdz.coef" = signif(stdz.coef(bc.sel.forw)))

# sort coefs by absolute value of their standardized regression coefficient (in decreasing order)
bc.sel.forw.coefs <- bc.sel.forw.coefs[order(-abs(bc.sel.forw.coefs$stdz.coef))]

# view results
bc.sel.forw.coefs
```

**Did at any point in this procedure occur that a variable entered the model and was later on discarded? If so which?**

When tracing back all step of stepAIC(), I found that stepAIC() only added variables, but never removed any. So I wonder whether this is possible to happen at all when using stepAIC().

#### Problem 2 (f)

Comparing the goodness of fit of model `bc.sel.back`(backwards elimination) and model `bc.sel.forw` (forward selection):
Given that the two models are not nested, we cannot use a likelihood ratio test.
But we can still use the AIC and BIC:

```{r}
#AIC
AIC(bc.sel.back)
AIC(bc.sel.forw)
```

Since model `bc.sel.back` has lower AIC, so it’s tempting to say it's the better model.
BUT:
```{r}
#BIC
BIC(bc.sel.back)
BIC(bc.sel.forw)
```

Model `bc.sel.forw` has lower BIC (which applies a stronger penalty for the use of additional predictors than AIC).
So, let's plot the AUCs of the both models:

```{r}
library(pROC)
par(mfrow=c(1,1))
roc(bc.dt$diagnosis[train.idx], bc.sel.back$fitted.values)
roc(bc.dt$diagnosis[train.idx], bc.sel.forw$fitted.values)
roc(bc.dt$diagnosis[train.idx], bc.sel.back$fitted.values, plot=TRUE, legacy.axes=TRUE, col="brown2", lwd = 1.4)
roc(bc.dt$diagnosis[train.idx], bc.sel.forw$fitted.values, plot=TRUE, legacy.axes=TRUE, add=TRUE, col="deepskyblue3", lwd = 1.4)
legend("bottomright", c("backwards elimination: AUC = 0.9916", "forward selection: AUC = 0.9891"), fill = c("brown2","deepskyblue3"))
```

Model `bc.sel.back` has the greater AUC, so it can be concluded it's the better model.

However, it should be noted that these measure only compare, how well our models fit our training data.
We do not yet know which model generalises better and performs better with the testing data.

#### Problem 2 (g)

#### Problem 2 (h)

### Problem 3

### Problem 4